{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concepts of NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['If', 'they', 'can', 'do', 'it', ',', 'so', 'can', 'you', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentence = 'If they can do it, so can you.'\n",
    "\n",
    "new_sentence = word_tokenize(text)\n",
    "\n",
    "print(new_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Stopwords For English and French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "\n",
      "['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle', 'en', 'et', 'eux', 'il', 'je', 'la', 'le', 'leur', 'lui', 'ma', 'mais', 'me', 'même', 'mes', 'moi', 'mon', 'ne', 'nos', 'notre', 'nous', 'on', 'ou', 'par', 'pas', 'pour', 'qu', 'que', 'qui', 'sa', 'se', 'ses', 'son', 'sur', 'ta', 'te', 'tes', 'toi', 'ton', 'tu', 'un', 'une', 'vos', 'votre', 'vous', 'c', 'd', 'j', 'l', 'à', 'm', 'n', 's', 't', 'y', 'été', 'étée', 'étées', 'étés', 'étant', 'étante', 'étants', 'étantes', 'suis', 'es', 'est', 'sommes', 'êtes', 'sont', 'serai', 'seras', 'sera', 'serons', 'serez', 'seront', 'serais', 'serait', 'serions', 'seriez', 'seraient', 'étais', 'était', 'étions', 'étiez', 'étaient', 'fus', 'fut', 'fûmes', 'fûtes', 'furent', 'sois', 'soit', 'soyons', 'soyez', 'soient', 'fusse', 'fusses', 'fût', 'fussions', 'fussiez', 'fussent', 'ayant', 'ayante', 'ayantes', 'ayants', 'eu', 'eue', 'eues', 'eus', 'ai', 'as', 'avons', 'avez', 'ont', 'aurai', 'auras', 'aura', 'aurons', 'aurez', 'auront', 'aurais', 'aurait', 'aurions', 'auriez', 'auraient', 'avais', 'avait', 'avions', 'aviez', 'avaient', 'eut', 'eûmes', 'eûtes', 'eurent', 'aie', 'aies', 'ait', 'ayons', 'ayez', 'aient', 'eusse', 'eusses', 'eût', 'eussions', 'eussiez', 'eussent']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = list(stopwords.words('english'))\n",
    "print(stop_words)\n",
    "print()\n",
    "stop_words = list(stopwords.words('french'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEMMED SENTENCE 1: sachin play cricket  \n",
      "STEMMED SENTENCE 2: sachin play cricket \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sentence_1 = 'Sachin is playing cricket'\n",
    "sentence_1 = word_tokenize(sentence_1)\n",
    "sentence_1 = [i for i in sentence_1 if not i in list(stopwords.words('english'))]\n",
    "\n",
    "sentence_2 = 'Sachin played cricket'\n",
    "sentence_2 = word_tokenize(sentence_2)\n",
    "sentence_2 = [i for i in sentence_2 if not i in list(stopwords.words('english'))]\n",
    "\n",
    "\n",
    "PS = PorterStemmer()\n",
    "\n",
    "stemmed_sentence_1 = ''\n",
    "stemmed_sentence_2 = ''\n",
    "\n",
    "for i in sentence_1:\n",
    "    stemmed_sentence_1 += PS.stem(i) + ' '\n",
    "\n",
    "for i in sentence_2:\n",
    "    stemmed_sentence_2 += PS.stem(i) + ' '\n",
    "\n",
    "print('STEMMED SENTENCE 1:',stemmed_sentence_1, '\\nSTEMMED SENTENCE 2:', stemmed_sentence_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "cactus\n",
      "goose\n",
      "rock\n",
      "python\n",
      "good\n",
      "best\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize(\"cats\"))\n",
    "print(lemmatizer.lemmatize(\"cacti\"))\n",
    "print(lemmatizer.lemmatize(\"geese\"))\n",
    "print(lemmatizer.lemmatize(\"rocks\"))\n",
    "print(lemmatizer.lemmatize(\"python\"))\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"best\", pos=\"a\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        50\n",
      "          1       0.94      0.94      0.94        50\n",
      "          2       0.94      0.94      0.94        50\n",
      "\n",
      "avg / total       0.96      0.96      0.96       150\n",
      "\n",
      "[[50  0  0]\n",
      " [ 0 47  3]\n",
      " [ 0  3 47]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "dataset = datasets.load_iris()\n",
    "\n",
    "model = GaussianNB()\n",
    "model.fit(dataset.data, dataset.target)\n",
    "expected = dataset.target\n",
    "predicted = model.predict(dataset.data)\n",
    "\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.confusion_matrix(expected, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9727427597955707\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "topics = ['rec.autos', 'rec.sport.hockey', 'sci.space', 'soc.religion.christian', 'talk.politics.guns']\n",
    "\n",
    "training = fetch_20newsgroups(subset = 'train', categories = topics, shuffle = True, random_state = 42)\n",
    "\n",
    "classifier = Pipeline([\n",
    "    ('vec', TfidfVectorizer(stop_words = stopwords.words('english'))),\n",
    "    ('cla', MultinomialNB())\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(training.data, training.target, test_size = 0.2, random_state = 33)\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "print('Accuracy:', classifier.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the query: weapons are not allowed in here\n",
      "Class Type: 4\n",
      "Class name: talk.politics.guns\n",
      "--------------------------------------------------\n",
      "For the query: thats a ferrari right?\n",
      "Class Type: 0\n",
      "Class name: rec.autos\n"
     ]
    }
   ],
   "source": [
    "query = ['weapons are not allowed in here']\n",
    "class_type = int(classifier.predict(query))\n",
    "print('For the query:',*query)\n",
    "print('Class Type:',class_type)\n",
    "print('Class name:', training.target_names[class_type])\n",
    "\n",
    "print('-'*50)\n",
    "\n",
    "query = ['thats a ferrari right?']\n",
    "class_type = int(classifier.predict(query))\n",
    "print('For the query:',*query)\n",
    "print('Class Type:',class_type)\n",
    "print('Class name:', training.target_names[class_type])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'icoord': [[35.0, 35.0, 45.0, 45.0],\n",
       "  [25.0, 25.0, 40.0, 40.0],\n",
       "  [15.0, 15.0, 32.5, 32.5],\n",
       "  [5.0, 5.0, 23.75, 23.75]],\n",
       " 'dcoord': [[0.0, 1.7320508075688772, 1.7320508075688772, 0.0],\n",
       "  [0.0, 2.0, 2.0, 1.7320508075688772],\n",
       "  [0.0, 2.23606797749979, 2.23606797749979, 2.0],\n",
       "  [0.0, 2.449489742783178, 2.449489742783178, 2.23606797749979]],\n",
       " 'ivl': [3, 5, 4, 1, 2],\n",
       " 'leaves': [2, 4, 3, 0, 1],\n",
       " 'color_list': ['b', 'b', 'b', 'b']}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "sentence_1 = 'sachin play'.split()\n",
    "sentence_2 = 'sachin father aditya'.split() \n",
    "sentence_3 = 'sachin and virat play india'.split()\n",
    "sentence_4 = 'dhoni work office'.split()\n",
    "sentence_5 = 'dravid study school'.split()\n",
    "\n",
    "\n",
    "terms = sorted(list(set(sentence_1 + sentence_2 + sentence_3 + sentence_4 + sentence_5)))\n",
    "\n",
    "# Generating the term frequency matrix.\n",
    "term_doc = [[0 for i in range(len(terms))] for j in range(5)]\n",
    "\n",
    "doc_no = 0\n",
    "\n",
    "# Iterating over the documents\n",
    "for i in [sentence_1, sentence_2, sentence_3, sentence_4, sentence_5]:\n",
    "    term_doc[doc_no] = [i.count(j) for j in terms]\n",
    "    doc_no += 1\n",
    "\n",
    "\n",
    "X = np.array(term_doc)\n",
    "\n",
    "linked = linkage(X, 'single')\n",
    "\n",
    "lo = 1\n",
    "hi = np.shape(X)[0] + 1\n",
    "\n",
    "labelList = range(lo, hi)\n",
    "\n",
    "plt.figure(figsize = (hi-lo, 5))\n",
    "dendrogram(linked,\n",
    "          orientation = 'top',\n",
    "          labels = labelList,\n",
    "          distance_sort = 'descending',\n",
    "          show_leaf_counts=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
